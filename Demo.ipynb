{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d12f94-9738-4ff0-be55-50472b2a5e06",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544d979-8096-456b-b57a-819f3378a17a",
   "metadata": {},
   "source": [
    "Какво е deployment? В изолация, всяко парче код е безполезно, включително и ML/AI. Deployment е процесът по \"отваряне\" на кода, така че други системи или потребители да могат да достъпват нашия софтуер "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13374622-03fd-4f0a-b195-ed3f94351eda",
   "metadata": {},
   "source": [
    "## Трениране на модел\n",
    "Ще натренираме модел, който използва отворен dataset, за да оцени дали дадено ревю за филм е позитивно или негативно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc1b60f-1b1d-4beb-a81f-dbea2c91b50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Невероятен!!! Ако искате да гледате нещо нестандартно, не се двоумете - това е филмът за вас ;))',\n",
       " 'Много добре направен :)',\n",
       " 'Определено най-добрият Хобит!',\n",
       " 'Муден старт, но нататък е хубав, ненатоварващ и приятен!',\n",
       " 'Невероятна поредица.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = \"sepidmnorozy/Bulgarian_sentiment\"\n",
    "ds = load_dataset(dataset, split=\"train\")\n",
    "ds['text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecef87-19a0-4e97-8e7c-a4215b54e347",
   "metadata": {},
   "source": [
    "Първо, трябва да превърнем текста в ML-readable формат, тоест във вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b01091-b514-49a1-a41c-a3947b5a6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([348,  51, 102, 229,  97,  57, 311, 159,  56, 165, 116, 135, 139, 116,\n",
       "         107, 184, 107,  67, 221, 286,   2,  35,  70,  71,  95, 136,  17, 214,\n",
       "           7, 164, 116,  28,   5, 115, 142,  61, 216, 114,  83, 178,  55,  30,\n",
       "          30, 347, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349,\n",
       "         349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349,\n",
       "         349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349],\n",
       "        dtype=torch.int32),\n",
       " tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocessing import Tokenizer, TokenizingDataset\n",
    "vocab_size = 350    # number of tokens\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit(ds['text'])\n",
    "train_ds = TokenizingDataset(ds, tokenizer)\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec18f3-4f0a-4dec-8793-bec93e241bf3",
   "metadata": {},
   "source": [
    "Тренираме модел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27daab9-e629-4136-8b20-54139bdfce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss=903.4081373512745\n",
      "epoch accuracy: 0.6572431633407243\n",
      "epoch_loss=881.0009697079659\n",
      "epoch accuracy: 0.8370288248337029\n",
      "epoch_loss=902.2710891962051\n",
      "epoch accuracy: 0.8398004434589801\n",
      "epoch_loss=902.9718952327967\n",
      "epoch accuracy: 0.8342572062084257\n",
      "epoch_loss=930.4109991937876\n",
      "epoch accuracy: 0.8433111603843311\n",
      "epoch_loss=939.8380871489644\n",
      "epoch accuracy: 0.8418329637841833\n",
      "epoch_loss=917.9347029030323\n",
      "epoch accuracy: 0.8442350332594235\n",
      "epoch_loss=921.8313402980566\n",
      "epoch accuracy: 0.8473762010347377\n",
      "epoch_loss=926.080776270479\n",
      "epoch accuracy: 0.8495934959349594\n",
      "epoch_loss=889.6333800554276\n",
      "epoch accuracy: 0.8516260162601627\n",
      "epoch_loss=911.191862154752\n",
      "epoch accuracy: 0.8562453806356245\n",
      "epoch_loss=886.0780337192118\n",
      "epoch accuracy: 0.8590169992609017\n",
      "epoch_loss=873.2165246680379\n",
      "epoch accuracy: 0.8608647450110865\n",
      "epoch_loss=895.6323585156351\n",
      "epoch accuracy: 0.8680709534368071\n",
      "epoch_loss=862.9621351677924\n",
      "epoch accuracy: 0.8669623059866962\n",
      "epoch_loss=857.3540775310248\n",
      "epoch accuracy: 0.8713968957871396\n",
      "epoch_loss=839.6861219517887\n",
      "epoch accuracy: 0.8741685144124168\n",
      "epoch_loss=837.6267134714872\n",
      "epoch accuracy: 0.8784183296378418\n",
      "epoch_loss=851.8564543165267\n",
      "epoch accuracy: 0.8804508499630451\n"
     ]
    }
   ],
   "source": [
    "from training_procedure import fit_model\n",
    "model = fit_model(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470978e-5455-43cf-b9a4-f18b19b35642",
   "metadata": {},
   "source": [
    "За да не се налага да тренираме всеки път, запазваме нашите тренирани компоненти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f03ab-9743-425d-89c7-5e08ef6c3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tokenizer.save(\"tokenizer.pkl\")\n",
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638f3bd-ad6f-408d-a79c-b0a22ac85946",
   "metadata": {},
   "source": [
    "За да използваме модела:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db621d-2f78-4803-a23b-ff45759014ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.IntTensor([[10, 15, 20, 25]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b68429-5904-4504-9434-02b4ad5a7589",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af174cf0-2f42-40fc-a3f6-6bf2673b0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SentimentInference():\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, inp: str):\n",
    "        tokens = self.tokenizer.tokenize(inp)[:self.model.context_size+1]\n",
    "        res = self.model(torch.IntTensor([tokens]))\n",
    "        return res.argmax().item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load():\n",
    "        m = torch.load(\"model.pt\")\n",
    "        t = Tokenizer.load(\"tokenizer.pkl\")\n",
    "        return SentimentInference(m, t)\n",
    "inference = SentimentInference.load()\n",
    "inference(\"скучен и глупав филм\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bba60-9d02-4a94-a76d-0f748e5b1e47",
   "metadata": {},
   "source": [
    "## Отваряне на достъпа\n",
    "Следва да пуснем този модел на прод. Нашето решение ще бъде:\n",
    "- в облака\n",
    "- безплатно\n",
    "- достъпно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ff938-a09e-41e7-af68-b45a30816433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from model import SentimentInference\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Review(BaseModel):\n",
    "    text: str\n",
    "    username: str\n",
    "\n",
    "app = FastAPI()\n",
    "inf = SentimentInference.load()\n",
    "\n",
    "@app.post(\"/sentiment\")\n",
    "async def infer_sentiment(req: Review):\n",
    "    res = inf(req.text)\n",
    "    response = \"negative\" if res == 0 else \"positive\"\n",
    "    return {\"sentiment\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606e9a-fc4c-4860-b566-01d430dec2f5",
   "metadata": {},
   "source": [
    "# Какво липсва в тази презентация?\n",
    "- automated deploy - понякога се налага периодично да тренираме нов модел и той автоматично да се деплойва вместо ръчно да го качваме по всички наши сървъри\n",
    "- security - всеки може да достъпва нашата виртуална машина. Нямаме никаква защита против атака, примерно DDOS\n",
    "- scale - моделът е прост и има само една инстанция. Обикновено се налага да имаме възможност да осигурим повече ресурси\n",
    "- docker и kubernetes - един сървиз би влязъл в контейнер под kubernetes. На практика, няма да се налага да правим ръчните команди по сетване на нова виртуалка, а нашия сървиз би бил по-secure и по-manageable откъм monitoring, logging, automated scaling, etc\n",
    "- stress test - на какъв брой рекуести може да издържи нашия сървиз\n",
    "- monitoring - как да разберем възможно най-бързо когато има проблем със сървиза или AWS\n",
    "- networking - важна тема, но реших да не разводнявам дискусията. Добра идея е поне да фиксираме адреса на нашия сървиз, така че да не го викаме по IP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
